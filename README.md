# Spelling-Correction-using-Tokenization-and-LevenshteinDistance
Input Book - Sense and Sensibility by Jane Austen
The spelling corrector follows every sentence and tokenizes each word of it based on whitespace. Special characters like punctuation and newlines are then re- tokenized and separated from the word start and end of the word to assemble an array of words. Additionally, all capitalizations are removed from the word and the indices of the capitalizations are stored. The list of clean words are passed through a word list, one at a time to check for spellings from a list of words. The word with the least Levenshtein distance from the misspelt word is treated as the one with correct spelling. In the case of a tie, the most frequently used word is treated as the true spelling. This method is repeated for the entire corrupted input string of Jane Austen’s Sense and Sensibility. Lastly the array of corrected words is then put together again using a join function while adding the special characters back and capitalizing the characters. An sample file, called bug_free.txt is the file after error check.

# Error Cases
The corrector will not work on words with a special character in the middle of the words (i.e. between alphabets) as that will be treated like an ordinary alphanumeric and passed on. This means if (don’t/won’t) is spelt as (dun’t/win’t) it will not be corrected. This is because the corrector treats only wholly alphanumeric words.
Another exception is when words are misspelt with the corrupted file deletes alphabets that result in a root word that is spelt correctly. e.g. the corrupted file could output shouldn’t as should’ with a deletion of n and t. The corrector will not correct this as post tokenization ‘should’ will be spelt correctly.
Another exception is that if multiple capitalizations exist in the corrupted word e.g. QHenry is detected, then the indices 1 and 2 are saved. However, when the word is corrected to Henry, the word will be capitalized as HEnry, as each index is capitalized. This is a necessary tradeoff as some words have multiple capitalizations in the original text, such as CHAPTER.
Lastly, it's important to note that the words that are substituted, appended or deleted will be corrected to the nearest word in terms of Levenshtein distance from the word list. This may not be completely accurate due to the limit of the list and lack of context of the remaining string. There are 4452 words in Jane Austen’s Sense and Sensibility that are absent from the Google 10,000 word corpus. For example, if repeatedly is spelt as replicatedly, it is 1 substitution and 2 deletions to correct the word. However, the corrector outputs replicatedly itself because the word list does not have the word ‘repeatedly’. Additionally, if context of the preceding strings isn’t analyzed, the correction can be useless. E.g. ‘He should do this’ is misspelt as ‘He whould do this’ as would is more frequently used as should. The ‘w' in is clearly supposed to be a ‘s’ but our system will correct it as would, which may not be coherent with the rest of the sentence.
At the same time the spelling corrector works well on word in the list like replica spelt at replico, potato spelt as potato, etc. We must consider that these are typing errors that are heuristically more frequent and therefore makes sense to correct using a simple corrector.

# Technical Details
There are a set of helper functions in the spellcheck file. The compute_lev function takes in two words as strings and returns the Levenshtein distance between the two. The find_replacement function takes a word, a dictionary mapping a words to relative frequencies, and an integer k. So the most frequent word will be mapped to 0 and so on. It uses compute_lev on every word in the dictionary and replaces the input word with the one with a minimum number of changes in the dictionary. The function will only accept the top k least amount of changes, and will output the original input word if the word with the least Levenshtein distance is greater than k. In the case of a tie, the function returns the most frequently used word.
The tokenize function takes a string and tokenizes it, returning tokens, lpunctuation, rpunctuation, capitalization, and isalpha. tokens is an array of cleaned phrases, having punctuation and capitalizations stripped from the word. lpunctuation is an array of punctuation to be appended to the left of the corresponding word at the same index in tokens. rpunctuation is an array of punctuation to be appended to the right of the corresponding word at the same index in tokens. For both lpunctuation and rpunctuation, the word punctuation is defined by python’s string.punctuation. capitalization is an array of arrays where each array is the capitalized indices in the original word. Finally, isalpha is an array of booleans indicating if the token in tokens is composed only of alphabetical characters after it has been cleaned.
The spellcheck function takes in data for a single index of each of tokens, lpunctuation, rpunctuation, capitalization, and isalpha, as well as a value k. spellcheck first checks isalpha and checks if it is true. If isalpha is false, then it returns the token. Otherwise, it calls find_replacement using the google 10,000 word corpus as the dictionary using the specified value of k. After receiving the replacement, the function retokenizes it by capitalizing the indices and adding lpunctuation and rpunctuation.
The final function, check, is what puts everything together and performs spell checking on a single input string. check calls tokenize to create the tokens, lpunctuation, rpunctuation, capitalization, and isalpha arrays. Then it generates a list of k which is the length of each token in tokens divided and floored by 3. After several tests, this was found to create the best result. Using these arrays, it calls spellcheck on every index using data from tokens, lpunctuation, rpunctuation, capitalization, and isalpha, and k. After the entire array is processed, the output array is joined by a whitespace, returning the corrected string.
Also to note, in the main function, we are using multiprocessing to speed of the data. Running the python script as is may not be optimal as the number of cores is different between computers.
